{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The theoretical train time complexity of a decision tree is O(n*mlog(n)) where n is number of training examples and the predict time complexity is O(d) where d is depth of the tree. From the graph we can see that the time complexity for my decision tree is O(n*mlog(n)) approximately as well as we can see the slope of the curve is more in the case when m was constant that in the case when n is constant this confirms our proposition. For prediction as well we see that when we two predictions have the same tree depth we are getting almost the same graph which is what we theoretically predicted and increases as the depth increases when we increase M and N.",
   "id": "c08498185de73b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd13bff6f9a143a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
