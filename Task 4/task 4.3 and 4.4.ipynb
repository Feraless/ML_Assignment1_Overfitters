{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "74fee0cc-c537-4fcb-9d92-9cd449320d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6109116f-af94-41ac-a475-983ac28c2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\LENOVO\\Desktop\\ML assignment 1\\api_key.txt', 'r') as f:\n",
    "    Groq_token = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19a08ff9-18d3-4910-9eb6-85e62844dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model = \"llama3-70b-8192\", api_key = Groq_token, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11960296-1a08-45cf-b97f-d09766aae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dictionary = {\n",
    "    1: 'WALKING',\n",
    "    2: 'WALKING UPSTAIRS',\n",
    "    3: 'WALKING DOWNSTAIRS',\n",
    "    4: 'SITTING',\n",
    "    5: 'STANDING',\n",
    "    6: 'LAYING'\n",
    "}\n",
    "\n",
    "activities = ['WALKING',\n",
    "    'WALKING UPSTAIRS',\n",
    "    'WALKING DOWNSTAIRS',\n",
    "    'SITTING',\n",
    "    'STANDING',\n",
    "    'LAYING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ef2e7b79-40ef-46ce-80ea-1949b7d66849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data collected from the wild\n",
    "\n",
    "dataset_dir = r'C:\\Users\\LENOVO\\Desktop\\ML assignment 1\\Collected Data\\Collected Data\\Processed files'\n",
    "files=os.listdir(dataset_dir)\n",
    "classes = {\"Walking\":1,\"Climbing_up\":2,\"Climbing_down\":3,\"Sitting\":4,\"Standing\":5,\"Laying\":6}\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for file in files:\n",
    "    df=pd.read_csv(os.path.join(dataset_dir,file))\n",
    "    X.append(df.values)\n",
    "\n",
    "    for i in classes:\n",
    "        if file.startswith(i):\n",
    "            Y.append(classes[i])\n",
    "            break\n",
    "\n",
    "X_collected = np.array(X)\n",
    "Y_collected = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "951a6fa9-6c48-407c-a58a-3992837c8453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 500, 3)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ddef29c3-3c20-4da2-941d-b207aeb14a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_collected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63f207-a452-402d-99c7-d809eab329c5",
   "metadata": {},
   "source": [
    "## Task 4.3\n",
    "#### Use the Few-Shot prompting method using UCI-HAR dataset to predict the activities that you performed. Ensure that both your examples and test query undergo similar preprocessing. How did the model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c9b47153-fa14-4be1-bf4a-f908e74b3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays obtained by MakeDataset.py \n",
    "# --> Train data compiled from the dataset (acc_x, acc_y, acc_z)\n",
    "x_train_dataset = np.load(r\"C:\\Users\\LENOVO\\Desktop\\ML assignment 1\\X_train.npy\")\n",
    "y_train_dataset = np.load(r\"C:\\Users\\LENOVO\\Desktop\\ML assignment 1\\y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9ff9ea77-cd3f-412f-877d-d96ab2adee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 500, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "20a218ba-180d-4758-b433-81909dc4dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54,)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fce79-b570-4905-8c31-ccda69cc2917",
   "metadata": {},
   "source": [
    "Activity Feature Vectors is a dictionary that contains activity labels and a few of its corresponding training examples (extracted from the dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bd3eb978-b3be-46de-a9d7-c0ae81f9c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_feature_vectors = {}\n",
    "for activity in activities:\n",
    "    activity_feature_vectors[activity] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56f6db87-3d34-4c32-82f7-3088377cd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_dataset)):\n",
    "    y = y_train_dataset[i]\n",
    "    activity_label = activity_dictionary[y]\n",
    "    activity_feature_vectors[activity_label].append(x_train_dataset[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5daa8a-880f-43ae-a2eb-10832120843f",
   "metadata": {},
   "source": [
    "The below few shot prompt takes in accelerometer readings from the dataset and predicts on the data that is collected by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e08d5bbf-0c64-43b5-a402-bd90636999ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample number: 1\n",
      "Few shot: LAYING\n",
      "Correct: WALKING DOWNSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 2\n",
      "Few shot: LAYING\n",
      "Correct: WALKING DOWNSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 3\n",
      "Few shot: LAYING\n",
      "Correct: WALKING DOWNSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 4\n",
      "Few shot: WALKING\n",
      "Correct: WALKING DOWNSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 5\n",
      "Few shot: LAYING\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 6\n",
      "Few shot: WALKING\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 7\n",
      "Few shot: LAYING\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 8\n",
      "Few shot: WALKING\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 9\n",
      "Few shot: LAYING\n",
      "Correct: LAYING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 10\n",
      "Few shot: LAYING\n",
      "Correct: LAYING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 11\n",
      "Few shot: LAYING\n",
      "Correct: LAYING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 12\n",
      "Few shot: STANDING\n",
      "Correct: LAYING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 13\n",
      "Few shot: STANDING\n",
      "Correct: SITTING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 14\n",
      "Few shot: LAYING\n",
      "Correct: SITTING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 15\n",
      "Few shot: LAYING\n",
      "Correct: SITTING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 16\n",
      "Few shot: LAYING\n",
      "Correct: SITTING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 17\n",
      "Few shot: STANDING\n",
      "Correct: STANDING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 18\n",
      "Few shot: LAYING\n",
      "Correct: STANDING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 19\n",
      "Few shot: LAYING\n",
      "Correct: STANDING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 20\n",
      "Few shot: LAYING\n",
      "Correct: STANDING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 21\n",
      "Few shot: STANDING\n",
      "Correct: WALKING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 22\n",
      "Few shot: LAYING\n",
      "Correct: WALKING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 23\n",
      "Few shot: LAYING\n",
      "Correct: WALKING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 24\n",
      "Few shot: LAYING\n",
      "Correct: WALKING\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "few_shot_predictions = []\n",
    "\n",
    "for test_data,test_label in zip(X_collected, Y_collected): \n",
    "    query_few_shot = f\"\"\"\n",
    "\n",
    "* You are a multiclass classifier model\n",
    "* You will be given below readings of humans performing one of the following six physical activities with the activity_labels: {activities}\n",
    "* The inputs are 500 timesteps of acc_x, acc_y, acc_z\n",
    "* Your task is to analyze the given input readings and predict which of the mentioned activities is the participant performing. \n",
    "\n",
    "Here are a few examples for you to learn from: {activity_feature_vectors}\n",
    "\n",
    "* Provide ONLY the activity name. NOTHING ELSE.\n",
    "For the input data below, VERY CAREFULLY identify the activity being performed: Input --> {test_data}\n",
    "\"\"\" \n",
    "\n",
    "    few_shot_answer = llm.invoke(query_few_shot)\n",
    "    few_shot_predictions.append(few_shot_answer.content)\n",
    "\n",
    "    i += 1\n",
    "    print(\"Test sample number:\", i)\n",
    "    print(\"Few shot:\",few_shot_answer.content)\n",
    "    # print(quantitative_query_few_shot)\n",
    "    \n",
    "    print(\"Correct:\", activity_dictionary[test_label])\n",
    "    # print(few_shot_predictions)    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ef6b9c1c-e3a6-472c-b79e-abba199110b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.array(y_true[:len(y_pred)])\n",
    "    y_pred = np.array(y_pred)\n",
    "    correct = y_true == y_pred\n",
    "    # print(correct)\n",
    "    return(np.sum(correct)/len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2f1de698-97bc-4f52-a4fb-6e2c7e49bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for label in list(np.array(Y_collected, dtype = \"int32\").ravel()):\n",
    "    y_test.append(activity_dictionary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2e964d94-1a96-486a-b881-eafbe497bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few shot accuracy: \n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Few shot accuracy: \")\n",
    "print(accuracy(y_test, few_shot_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc7007-360d-44c9-a44f-76a4b5eed7a3",
   "metadata": {},
   "source": [
    "The accuracy is equivalent to random prediction. <br>\n",
    "o The reason for this might be the fact that the orientation in which we used the device might be different to the orientation that the authors used while curating their dataset <br>\n",
    "o OR due to some differences in the device or software used while taking the readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414d383-324c-4dad-94fc-edb8516306f2",
   "metadata": {},
   "source": [
    "## Task 4.4\n",
    "#### Use the Few-Shot prompting method using the data you collected to predict the activities that you performed. Adopt proper processing methods as needed. How did the model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8e48a-5b65-45e2-908b-c639b4a2ef6a",
   "metadata": {},
   "source": [
    "Random shuffling the collected values in order to get train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf2c7816-8691-48b9-aa43-373ff396ceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20, 12, 3, 7, 9, 2, 22, 15, 10, 8, 6, 19, 23, 14, 17, 18, 11, 21, 16, 4, 0, 13, 5]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_indices = list(range(X_collected.shape[0]))\n",
    "random.shuffle(train_indices)\n",
    "print(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "41311b3f-c76f-4608-b014-6e76f7ee8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_collected = X_collected[train_indices[:17], :, :]\n",
    "y_train_collected = Y_collected[train_indices[:17]]\n",
    "\n",
    "x_test_collected = X_collected[train_indices[17:], :, :]\n",
    "y_test_collected = Y_collected[train_indices[17:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "153bbea0-e69b-42c7-98b8-37c883be2eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 4, 3, 2, 6, 3, 1, 4, 6, 6, 2, 5, 1, 4, 5, 5])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "36a08507-7800-4b49-90b3-c277a10a837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_feature_vectors = {}\n",
    "for activity in activities:\n",
    "    activity_feature_vectors[activity] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1cfe0dbe-03ff-4b3d-a1bd-5ae24c2a8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_collected)):\n",
    "    y = y_train_collected[i]\n",
    "    activity_label = activity_dictionary[y]\n",
    "    activity_feature_vectors[activity_label].append(x_train_collected[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe4eb1-1799-48c8-b4fd-88b815413736",
   "metadata": {},
   "source": [
    "The below few shot prompt takes in accelerometer readings from the collected data and predicts on the data that is collected by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0d27304-8db1-45ad-a575-16d6661a192e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample number: 1\n",
      "Few shot: LAYING\n",
      "Correct: LAYING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 2\n",
      "Few shot: WALKING\n",
      "Correct: WALKING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 3\n",
      "Few shot: STANDING\n",
      "Correct: STANDING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 4\n",
      "Few shot: WALKING\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 5\n",
      "Few shot: WALKING DOWNSTAIRS\n",
      "Correct: WALKING DOWNSTAIRS\n",
      "---------------------------------------------------------\n",
      "Test sample number: 6\n",
      "Few shot: STANDING\n",
      "Correct: SITTING\n",
      "---------------------------------------------------------\n",
      "Test sample number: 7\n",
      "Few shot: WALKING DOWNSTAIRS\n",
      "Correct: WALKING UPSTAIRS\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "few_shot_predictions = []\n",
    "\n",
    "for test_data,test_label in zip(x_test_collected, y_test_collected): \n",
    "    query_few_shot = f\"\"\"\n",
    "\n",
    "* You are a multiclass classifier model\n",
    "* You will be given below readings of humans performing one of the following six physical activities with the activity_labels: {activities}\n",
    "* The inputs are 500 timesteps of acc_x, acc_y, acc_z\n",
    "* Your task is to analyze the given input readings and predict which of the mentioned activities is the participant performing. \n",
    "\n",
    "Here are a few examples for you to learn from: {activity_feature_vectors}\n",
    "\n",
    "* Provide ONLY the activity name. NOTHING ELSE.\n",
    "For the input data below, VERY CAREFULLY identify the activity being performed: Input --> {test_data}\n",
    "\"\"\" \n",
    "\n",
    "    few_shot_answer = llm.invoke(query_few_shot)\n",
    "    few_shot_predictions.append(few_shot_answer.content)\n",
    "\n",
    "    i += 1\n",
    "    print(\"Test sample number:\", i)\n",
    "    print(\"Few shot:\",few_shot_answer.content)\n",
    "    # print(quantitative_query_few_shot)\n",
    "    \n",
    "    print(\"Correct:\", activity_dictionary[test_label])\n",
    "    # print(few_shot_predictions)    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f51ee8c2-bb03-4298-9b97-4f3d4268fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for label in list(np.array(y_test_collected, dtype = \"int32\").ravel()):\n",
    "    y_test.append(activity_dictionary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6bec562-12b0-4883-8399-7bcbf288b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few shot accuracy: \n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Few shot accuracy: \")\n",
    "print(accuracy(y_test, few_shot_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582f3b5-ab92-4933-956d-4c72b713729c",
   "metadata": {},
   "source": [
    "We get an accuracy of ~57% which is decent but one of the limitations for this is that our test set is too small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
